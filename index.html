<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.551">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2024-04-24">
<meta name="keywords" content="AI Security, Jailbreak Tactics, Taxonomy Development, Adversarial Techniques, Generative AI Vulnerabilities">

<title>Unveiling Risks in AI Systems: Taxonomic Insights into Jailbreak Tactics</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script async="" src="https://hypothes.is/embed.js"></script>
<script>
  window.document.addEventListener("DOMContentLoaded", function (_event) {
    document.body.classList.add('hypothesis-enabled');
  });
</script>


<meta name="citation_title" content="Unveiling Risks in AI Systems: Taxonomic Insights into Jailbreak Tactics">
<meta name="citation_abstract" content="Large language models (LLMs), enabled by advances in generative AI, hold immense potential but also face risks from adversarial techniques such as jailbreaking that bypass model restrictions. Jailbreak prompts exploit vulnerabilities to elicit harmful responses, violating ethics and safety. However, the AI community lacks a rigorous taxonomy characterizing diverse jailbreak techniques. This research helps fill this gap through methodical taxonomy development and validation. Probabilistic topic modeling (LDA) provided an initial automatic analysis of themes in a corpus of 90 real-world jailbreak prompts from online sources. Conversational AI assistants interpreted the topics in plain language, and the authors leveraged domain knowledge to organize these into a multi-tiered taxonomy delineating relationships. The utility of the taxonomy was validated through manual topic tagging, checks of representative documents, and comparisons with modeling outputs. The resulting taxonomy categorizes jailbreak prompts into a hierarchy of interpretable categories and themes, aiding in their analysis. This aids strategic efforts to detect risks, enhance protections, and balance innovation with responsibility in generative AI systems against irresponsible attacks. By providing a structured approach to identifying and categorizing jailbreak prompts, this taxonomy not only enhances security measures but also informs ongoing developments in ethical AI practices, inviting constructive community feedback to further improve this important step towards safer, more reliable LLMs.
">
<meta name="citation_keywords" content="AI Security,Jailbreak Tactics,Taxonomy Development,Adversarial Techniques,Generative AI Vulnerabilities">
<meta name="citation_author" content="Michael Borck">
<meta name="citation_author" content="Nik Thompson">
<meta name="citation_publication_date" content="2024-04-24">
<meta name="citation_cover_date" content="2024-04-24">
<meta name="citation_year" content="2024">
<meta name="citation_online_date" content="2024-04-24">
<meta name="citation_language" content="en">
<meta name="citation_journal_title" content="BARG Curtin University">
<meta name="citation_reference" content="citation_title=Literate programming;,citation_author=Donald E. Knuth;,citation_publication_date=1984-05;,citation_cover_date=1984-05;,citation_year=1984;,citation_fulltext_html_url=https://doi.org/10.1093/comjnl/27.2.97;,citation_issue=2;,citation_doi=10.1093/comjnl/27.2.97;,citation_issn=0010-4620;,citation_volume=27;,citation_journal_title=Comput. J.;,citation_publisher=Oxford University Press, Inc.;">
</head>

<body>

<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Unveiling Risks in AI Systems: Taxonomic Insights into Jailbreak Tactics</h1>
          </div>

    
    <div class="quarto-title-meta-container">
      <div class="quarto-title-meta-column-start">
            <div class="quarto-title-meta-author">
          <div class="quarto-title-meta-heading">Authors</div>
          <div class="quarto-title-meta-heading">Affiliation</div>
          
                <div class="quarto-title-meta-contents">
            <p class="author">Michael Borck <a href="mailto:michael.borck@curtin.edu.au" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> <a href="https://orcid.org/0000-0002-0950-6396" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a></p>
          </div>
                <div class="quarto-title-meta-contents">
                    <p class="affiliation">
                        Business Information Systems, Curtin University, Perth Australia
                      </p>
                  </div>
                      <div class="quarto-title-meta-contents">
            <p class="author">Nik Thompson <a href="mailto:nik.thompson@curtin.edu.au" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> <a href="https://orcid.org/0000-0002-0783-1371" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a></p>
          </div>
                <div class="quarto-title-meta-contents">
                    <p class="affiliation">
                        Business Information Systems, Curtin University, Perth Australia
                      </p>
                  </div>
                    </div>
        
        <div class="quarto-title-meta">

                      
                <div>
            <div class="quarto-title-meta-heading">Published</div>
            <div class="quarto-title-meta-contents">
              <p class="date">April 24, 2024</p>
            </div>
          </div>
          
                
              </div>
      </div>
      <div class="quarto-title-meta-column-end quarto-other-formats-target">
      <div class="quarto-alternate-formats"><div class="quarto-title-meta-heading">Other Formats</div><div class="quarto-title-meta-contents"><p><a href="index.docx"><i class="bi bi-file-word"></i>MS Word</a></p></div><div class="quarto-title-meta-contents"><p><a href="index-meca.zip" data-meca-link="true"><i class="bi bi-archive"></i>MECA Bundle</a></p></div></div></div>
    </div>

    <div>
      <div class="abstract">
        <div class="block-title">Abstract</div>
        <p>Large language models (LLMs), enabled by advances in generative AI, hold immense potential but also face risks from adversarial techniques such as jailbreaking that bypass model restrictions. Jailbreak prompts exploit vulnerabilities to elicit harmful responses, violating ethics and safety. However, the AI community lacks a rigorous taxonomy characterizing diverse jailbreak techniques. This research helps fill this gap through methodical taxonomy development and validation. Probabilistic topic modeling (LDA) provided an initial automatic analysis of themes in a corpus of 90 real-world jailbreak prompts from online sources. Conversational AI assistants interpreted the topics in plain language, and the authors leveraged domain knowledge to organize these into a multi-tiered taxonomy delineating relationships. The utility of the taxonomy was validated through manual topic tagging, checks of representative documents, and comparisons with modeling outputs. The resulting taxonomy categorizes jailbreak prompts into a hierarchy of interpretable categories and themes, aiding in their analysis. This aids strategic efforts to detect risks, enhance protections, and balance innovation with responsibility in generative AI systems against irresponsible attacks. By providing a structured approach to identifying and categorizing jailbreak prompts, this taxonomy not only enhances security measures but also informs ongoing developments in ethical AI practices, inviting constructive community feedback to further improve this important step towards safer, more reliable LLMs.</p>
      </div>
    </div>

    <div>
      <div class="keywords">
        <div class="block-title">Keywords</div>
        <p>AI Security, Jailbreak Tactics, Taxonomy Development, Adversarial Techniques, Generative AI Vulnerabilities</p>
      </div>
    </div>

    <div class="quarto-other-links-text-target">
    </div>  </div>
</header><div id="quarto-content" class="page-columns page-rows-contents page-layout-article toc-left">
<div id="quarto-sidebar-toc-left" class="sidebar toc-left">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">1</span> Introduction</a></li>
  <li><a href="#literature-review" id="toc-literature-review" class="nav-link" data-scroll-target="#literature-review"><span class="header-section-number">2</span> Literature Review</a>
  <ul class="collapse">
  <li><a href="#key-issues-and-risk-factors-for-llms" id="toc-key-issues-and-risk-factors-for-llms" class="nav-link" data-scroll-target="#key-issues-and-risk-factors-for-llms"><span class="header-section-number">2.0.1</span> Key Issues and Risk Factors for LLMs</a></li>
  <li><a href="#prior-work-on-taxonomy-development" id="toc-prior-work-on-taxonomy-development" class="nav-link" data-scroll-target="#prior-work-on-taxonomy-development"><span class="header-section-number">2.0.2</span> Prior Work on Taxonomy Development</a></li>
  </ul></li>
  <li><a href="#methodology" id="toc-methodology" class="nav-link" data-scroll-target="#methodology"><span class="header-section-number">3</span> Methodology</a>
  <ul class="collapse">
  <li><a href="#data-collection" id="toc-data-collection" class="nav-link" data-scroll-target="#data-collection"><span class="header-section-number">3.1</span> Data Collection</a></li>
  </ul></li>
  <li><a href="#results" id="toc-results" class="nav-link" data-scroll-target="#results"><span class="header-section-number">4</span> Results</a>
  <ul class="collapse">
  <li><a href="#prompts" id="toc-prompts" class="nav-link" data-scroll-target="#prompts"><span class="header-section-number">4.1</span> Prompts</a></li>
  <li><a href="#topic-modelling" id="toc-topic-modelling" class="nav-link" data-scroll-target="#topic-modelling"><span class="header-section-number">4.2</span> Topic Modelling</a></li>
  <li><a href="#topic-validation" id="toc-topic-validation" class="nav-link" data-scroll-target="#topic-validation"><span class="header-section-number">4.3</span> Topic Validation</a></li>
  <li><a href="#final-taxonomy" id="toc-final-taxonomy" class="nav-link" data-scroll-target="#final-taxonomy"><span class="header-section-number">4.4</span> Final Taxonomy</a></li>
  </ul></li>
  <li><a href="#discussion" id="toc-discussion" class="nav-link" data-scroll-target="#discussion"><span class="header-section-number">5</span> Discussion</a></li>
  <li><a href="#future-work" id="toc-future-work" class="nav-link" data-scroll-target="#future-work"><span class="header-section-number">6</span> Future Work</a>
  <ul class="collapse">
  <li><a href="#limitations" id="toc-limitations" class="nav-link" data-scroll-target="#limitations"><span class="header-section-number">6.0.1</span> Limitations</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">7</span> Conclusion</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references"><span class="header-section-number">8</span> References</a></li>
  </ul>
<div class="quarto-alternate-notebooks"><h2>Notebooks</h2><ul><li><a href="src/step-1-creating-descriptors-preview.html"><i class="bi bi-journal-code"></i>step-1-creating-descriptors.md</a></li><li><a href="src/step-2-critiquing-and-merging-concepts-preview.html"><i class="bi bi-journal-code"></i>step-2-critiquing-and-merging-concepts.md</a></li><li><a href="results/taxonomy-preview.html"><i class="bi bi-journal-code"></i>taxonomy.md</a></li><li><a href="results/claude-preview.html"><i class="bi bi-journal-code"></i>Claude</a></li><li><a href="results/gpt-4-preview.html"><i class="bi bi-journal-code"></i>GPT-4</a></li><li><a href="results/consensus_results-preview.html"><i class="bi bi-journal-code"></i>GPT-4 Final Topic List</a></li></ul></div></nav>
</div>
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
</div>
<main class="content quarto-banner-title-block" id="quarto-document-content">



  


<section id="introduction" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Introduction</h1>
<p>Generative AI, a significant branch of artificial intelligence (AI), has garnered widespread attention and stirred a fervent debate. At the core of this technology, large language models (LLMs), empowered by their unique capability to generate novel content in response to textual prompts.</p>
<p>Adversarial agents have been relentlessly testing the fortitude of these LLMs, capitalising on their vulnerabilities to launch attacks. Yet, our inadequate understanding of these adversarial prompts and the potential risks, ethics, alignment, and safety challenges they carry. This paper addresses one such critical adversarial prompting technique: jailbreaking.</p>
<p>The concept of a jailbreak, borrowed from the world of software systems, refers to a process where hackers ingeniously reverse engineer a system to exploit its vulnerabilities and gain undue privileges. When applied to LLMs, jailbreaking signifies the circumvention of model limitations and restrictions. The double-edged sword of this technique is becoming increasingly evident, with developers and researchers using it to both unlock the full potential of LLMs and violate ethical and legal norms (Li et al.&nbsp;2023). See Figure 1 (Piedrfatia 2022) for an example of a jailbreak prompt.</p>
<p>Controversially, the AI and security communities still lack a comprehensive taxonomy of jailbreak prompts, a deficiency that hampers our ability to safeguard these advanced systems. Not all jailbreaks are created equal. They range from harmless attempts to tweak basic customisations, to aggressive full-access jailbreaks that pose a significant risk. Understanding the specifics of each type is essential to proactively address the most severe threats.</p>
<p>This paper puts forth a comprehensive taxonomy of jailbreak prompts. By doing so, we aim to provide a strategic perspective to manage the risks, ethics, and safety aspects of LLMs, striking a balance between protecting against jailbreaking harms and fostering ethical innovation within the generative AI domain.</p>
<p>Figure Example Jailbreak prompt (Piedrfatia 2022)</p>
</section>
<section id="literature-review" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Literature Review</h1>
<section id="key-issues-and-risk-factors-for-llms" class="level3" data-number="2.0.1">
<h3 data-number="2.0.1" class="anchored" data-anchor-id="key-issues-and-risk-factors-for-llms"><span class="header-section-number">2.0.1</span> Key Issues and Risk Factors for LLMs</h3>
<p>Understanding the robustness and generalisation of language models is instrumental in our understanding of AI vulnerabilities. The ability of these models to handle diverse inputs and generalise their understanding plays a crucial role in their application (Carlini and Wagner 2017; Wang et al.&nbsp;2023a; Wang et al.&nbsp;2023b; Zhang et al.&nbsp;2019). These studies reveal the complexity of achieving language model generalisation and its potential implications on AI vulnerabilities. They highlight the need for further research and development to address the vulnerabilities of these models.</p>
<p>Recent research has revealed significant vulnerabilities in LLMs that could pose serious security and ethical risks if exploited. Studies have shown that LLMs can be manipulated to generate harmful or unreliable outputs by embedding malicious triggers into their training data or inputs (Xu et al.&nbsp;2022). Attackers can also actively manipulate LLMs using techniques such as natural language adversarial examples (Alzantot et al.&nbsp;2018) and targeted prompts that deceive the model into making incorrect predictions (Maus et al.&nbsp;2023). This exposes the susceptibility of LLMs to potential misuse, ranging from spreading misinformation to causing financial or physical harm (Carlini and Wagner 2018). Understanding the nature and scope of these vulnerabilities is crucial to develop effective safeguards for responsible AI development. A comprehensive taxonomy categorising different exploits could provide crucial strategic insights (Yang et al.&nbsp;2022) enabling stakeholders to manage risks, align values, and ensure the safety and reliability of AI systems. Clearly, continued research into shoring up vulnerabilities alongside taxonomies characterising exploits is both vital to securing LLMs against potential misuse in high-stakes domains.</p>
<p>Adversarial attack techniques, employed to test and reveal weaknesses in machine learning models, including language models, offer a panoply of methods, each with their own objectives and implications. Central to this are the gradient-based attacks (Ebrahimi et al.&nbsp;2017), word substitution attacks (Wallace et al.&nbsp;2019; Yu et al.&nbsp;2022), and black-box attacks (Papernot et al.&nbsp;2017), which collectively aim to manipulate models by injecting perturbations or generating adversarial examples that confuse the models. Collectively, these attack techniques elucidate the vulnerabilities of language models, underscoring the importance of considering adversarial attacks during their development and evaluation. This narrative further reinforces the need for robust defences to protect against these attacks and ensure the reliability of machine learning models in practical settings.</p>
<p>Liu et al.&nbsp;(2023) empirical study reveals that carefully crafted jailbreak prompts can successfully circumvent restrictions imposed on LLMs, with privilege escalation prompts incorporating multiple techniques having higher success rates in bypassing protections. The study also finds variability in protection strength across LLM models, emphasising the challenges of generating robust defences and aligning policies with laws and ethics to minimise harm.</p>
<p>AI jailbreaks pose critical threats to user privacy and system security that must be addressed (Alauddin et al.&nbsp;2021). Sensitive personal information extracted through jailbreaks can enable fraud, identity theft, and other exploitation, severely undermining individuals’ data sanctity (Alauddin et al.&nbsp;2021). Healthcare represents a salient use case, as compromised patient data undercut trust in the system and endangers wellbeing (Seh et al.&nbsp;2021). Moreover, complex AI systems used across sectors often lack accountability and explainability, thus impeding transparency around decision-making processes (Doshi-Velez et al.&nbsp;2017). Overall, jailbreaks jeopardise confidentiality through data breaches, misuse of personal details, and compromised privacy. Robust security measures, explainability, and accountability frameworks are critical to protect against these far-reaching dangers.</p>
<p>The emergence of jailbreak attacks on AI systems has raised concerns about the responsible use of generative AI models (Wu et al.&nbsp;2023b). Without proper defences, LLMs can produce biased, offensive or dangerous content in response to malicious instructions (Au Yeung et al.&nbsp;2023), potentially spreading misinformation or promoting harmful behaviours, which damages trust in AI systems, especially in sensitive domains like healthcare (Alauddin et al.&nbsp;2021). To address this, researchers have developed defensive techniques such as the “System-Mode Self-Reminder”, which significantly reduces the success rate of jailbreak attacks against ChatGPT (Wu et al.&nbsp;2023a). Implementing safeguards will be crucial to ensure responsible and secure AI development, as the risks extend beyond chatbots to recommendation systems and other generative AI applications (Kim et al.&nbsp;2021). While defensive techniques help, continued research and vigilance are needed. The dynamic interplay between a comprehensive prompt taxonomy development and empirical defence testing can accelerate progress in responsible AI that resists irresponsible attacks. A strong taxonomy provides a strategic lens to engineer defences, while the analysis of defence weaknesses further bolsters the taxonomy - crucial to secure, ethical AI.</p>
<p>Research has shown that adversarial examples crafted to fool one AI model frequently transfer to deceive other models, even across different architectures and training sets (Elsayed et al.&nbsp;2018; Kurakin et al.&nbsp;2016; Papernot et al.&nbsp;2016). This enables black-box attacks without knowledge of the target model’s parameters (Kurakin et al.&nbsp;2016) and means a model’s robustness depends on the vulnerabilities of others. Defending against transferable attacks requires resilient models trained on diverse adversarial data (Elsayed et al.&nbsp;2018; Kurakin et al.&nbsp;2016). However, transferability varies based on factors such as attack method and model architecture (Kurakin et al.&nbsp;2016; Yuan et al.&nbsp;2020). A comprehensive taxonomy of adversarial techniques could aid targeted defence development by characterising the transferability of different exploit categories. Understanding the nuances of transferability is key to engineering robust models and reliable real-world deployment.</p>
<p>AI jailbreaks present complex ethical dilemmas, as generative models have huge potential but can also be misused to generate harmful content (Gordon et al.&nbsp;2022). The key issues are fairness and bias, as AI often perpetuates existing prejudices from training data, potentially amplifying discrimination (Khan et al.&nbsp;2022). Transparency around capabilities, limitations, and decision-making is also crucial so users can evaluate AI reasoning and ensure accountability (Kerr et al.&nbsp;2020). An ethical framework is needed to guide developers, companies, and regulators in responsibly designing, deploying, and overseeing AI via principles of transparency, fairness, and accountability. This framework must consider risks and prevent misuse while balancing free expression (Khan et al.&nbsp;2022). Jailbreaking raises pressing ethical questions that require collaborative efforts among stakeholders to realise AI’s potential while upholding ethics through guidelines, regulations, and oversight. Analysis of taxonomy categories could reveal gaps in the current ethical governance of AI systems, guiding the development of more comprehensive frameworks, regulations, and oversight.</p>
<p>Safeguarding LLM vulnerabilities to adversarial text examples, sparks an ongoing battle between attacks and defences (Kurakin et al.&nbsp;2016). Initial mitigations such as adversarial training helped but were circumvented by increasingly sophisticated attacks (Aliyu et al.&nbsp;2022). Other techniques like defensive distillation also had limitations, sometimes improving performance on adversarial examples over real text (Kurakin et al.&nbsp;2016)! Researchers responded creatively, using blended adversarial data (Si et al.&nbsp;2021) and mixed representations to expand model robustness (Si et al.&nbsp;2021)(Si et al.&nbsp;2021). However, exponential attack possibilities persist, so the pursuit of resilient language models continues. As attacks grow more cunning, defenders employ adversarial training, distillation, and other techniques to meet the challenge. Constructing a comprehensive taxonomy of attacks and defences would aid the strategic development of robust models resilient to known and emerging threats.</p>
<p>Educating end-users, developers, and policymakers is crucial to enhance understanding of AI jailbreak vulnerabilities, risks, and countermeasures, empowering them to prevent incidents (Doumat et al.&nbsp;2022; Ninaus and Sailer 2022). Education should provide both general AI literacy and profession-specific training on bypass vulnerabilities. Developers need awareness of potential loopholes to avoid exploitation, while involving users promotes transparency in limitations (Ninaus and Sailer 2022). Policymakers require knowledge to develop effective regulations mitigating risks. Beyond education, raising multi-stakeholder awareness via campaigns and conferences is key to ensuring that all have the information needed to prevent jailbreaks and enable responsible AI use. Tailored education and comprehensive awareness efforts are essential to equip stakeholders with the understanding to proactively address jailbreak threats.</p>
<p>As AI systems continue to advance, so too do the threats to using jailbreak techniques designed to manipulate them for harmful ends. If action is not taken to comprehensively characterise and mitigate these dangers, generative models risk becoming tools for spreading misinformation, perpetuating discrimination, and enabling cybercrime. Without strategic oversight, the very technologies set to revolutionise fields from healthcare to education could violate privacy and ethics in deeply troubling ways. Developing a taxonomy that systematically maps the diversity of jailbreak prompts alongside tailored safeguards represents one crucial step toward averting these costs and risks. By codifying emerging threats and security vulnerabilities, we can arm developers, regulators, and society with the insights needed to secure AI systems against irresponsible attacks. The alternative is to ignore the writing on the wall and invite dire consequences in the name of progress. Comprehensive jailbreak taxonomy is a significant positive step toward safer LLMs.</p>
</section>
<section id="prior-work-on-taxonomy-development" class="level3" data-number="2.0.2">
<h3 data-number="2.0.2" class="anchored" data-anchor-id="prior-work-on-taxonomy-development"><span class="header-section-number">2.0.2</span> Prior Work on Taxonomy Development</h3>
<p>Research across diverse fields underscores the intricate process of developing rigorous taxonomies and provides guidance for methodology. Examples span implementing a knowledge framework (Field et al.&nbsp;2014), teaching practices in science education (Couch et al.&nbsp;2015), business analytics applications (Ko and Gillani 2020), information systems design (Kundisch et al.&nbsp;2021; Omair and Alturki 2020), self-service business intelligence (Passlick et al.&nbsp;2023), Industrial IoT threats (Abbas et al.&nbsp;2020), mobile app development (Werth et al.&nbsp;2019), and program evaluation (Stevahn et al.&nbsp;2005). Collectively, these studies demonstrate varied approaches to systematic taxonomy construction using techniques like structured reviews, citation analysis, design science paradigms and iterative development. They provide frameworks and operational recommendations that can inform taxonomy design across disciplines, including the current effort to develop a rigorous taxonomy of jailbreak prompts for generative AI systems.</p>
<p>There are crucial distinctions between the standard Linnaean system for classifying living things and taxonomies used in computing applications. Unlike the rigid, hierarchical structure of Linnaean taxonomies, computing taxonomies allow more flexibility. A single taxon can have multiple parent terms, rather than being restricted to one branch of the taxonomy. Taxons may also relate to multiple areas of the taxonomy, not just a single location. Additionally, computing taxonomies emphasise lexical synonyms more than traditional biological taxonomies. Overall, computing taxonomies have different priorities and needs compared to classical biological taxonomies.(Clarke 2012)</p>
<p>Liu et al.&nbsp;(2023) followed a qualitative thematic analysis with independent classification by three authors, followed by deliberation and refinement of the taxonomy. They collected 78 real-world jailbreak prompts from online sources and developed a categorisation model to classify prompts into 10 patterns across 3 types (pretending, attention shifting, privilege escalation). In this study, we use topic modelling to provide a lexical semantic analysis as opposed to conventional thematic analysis.</p>
</section>
</section>
<section id="methodology" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Methodology</h1>
<p>Rodriguez and Storer (2020) demonstrate that topic modelling can be used in a similar way to initial qualitative analysis, with some key distinctions: Both topic modelling and conventional thematic analysis are inductive and focus on understanding phenomena, but topic modelling provides a lexical semantic analysis, while qualitative analysis offers a compositional semantic analysis.</p>
<p>We followed the Extended Taxonomy Design Process (ETDP) methodology (Kundisch et al.&nbsp;2021), which extends the approach proposed by Nickerson et al.&nbsp;Nickerson et al.&nbsp;(2013) and encourages guiding the decision process when constructing a taxonomy.</p>
<p>Our methodology to construct a taxonomy involved using a probabilistic LDA(Blei et al.&nbsp;2003) approach to obtain initial topics. However, topic modelling results can be difficult to interpret. To improve clarity, we leveraged two conversational AI chatbots, GPT-4 and Claude, to independently summarise each topic in one word. Then each critiqued each other’s word choices through iterative discussion until reaching consensus on the most appropriate term. The chatbots were guided and provided a context that we were categorising jailbreak prompts and to align the interpretations with the practical application of detecting or preventing jailbreak prompts including details of the previous taxonomy. This generated intuitive, one-word topic labels. The authors then categorised the next level of the taxonomy hierarchy based on the relationships between these one-word topic labels. This created a multi-tiered structure with general themes at the top, divided into more granular sub-themes.</p>
<p>This systematic process combined the strengths of probabilistic topic modelling and conversational AI and incorporated human judgments to generate an intuitive taxonomy. The chatbots produced clear topic labels, while the authors used domain knowledge to categorise them into a hierarchy. This blended automated analysis with human refinement to balance complexity and interpretability (Chang et al.&nbsp;2009). The resulting taxonomy organises jailbreak prompts according to interpretable thematic connections.</p>
<p>Validation of the developed taxonomy was conducted through both manual and automated processes. First, all prompts in the corpus were manually tagged with appropriate topics based on the taxonomy structure. A sample of representative documents was then selected to verify alignment between assigned topics and prompt content. In addition, a random subset of prompts was chosen and classified by the authors using the taxonomy. These manual topic labels were compared to those automatically generated by the topic modelling, with full agreement observed.</p>
<p>Taken together, these validation approaches provided human-centered and algorithmic confirmation of taxonomy accuracy and applicability. The ability to reliably assign taxonomy topics to unclassified prompts demonstrates its utility for organising future data. By triangulating results across manual tagging, verification of representative documents, and comparison to automated topic modelling, the taxonomy was found to robustly capture semantic themes and connections within the jailbreak prompt corpus.</p>
<section id="data-collection" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="data-collection"><span class="header-section-number">3.1</span> Data Collection</h2>
<p>The jailbreak prompt corpus utilised in this analysis was constructed through multi-source data collection from publicly available online domains. Initial web scraping compiled 177 unique generative AI prompts from websites, YouTube, GitHub repositories, and comments. Exact duplicate prompts were then removed to mitigate potential bias and ensure a diverse corpus. This deduplication process yielded a final refined set of 90 distinct jailbreak prompts suitable for in-depth investigation.</p>
<p>Sourcing content across website communities, social media platforms, code repositories, and discussion forums provided heterogeneity in prompt styles and creators. This diversity limits over-representation of any singular perspective, supporting wider generalisability of findings. The combination of expansive sourcing and duplicate filtering enabled the creation of a robust, quality corpus for rigorous thematic analysis of jailbreak prompts.</p>
</section>
</section>
<section id="results" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Results</h1>
<section id="prompts" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="prompts"><span class="header-section-number">4.1</span> Prompts</h2>
<p>Preliminary analysis of a corpus containing 90 jailbreak prompts, with an average of 433 words per document, reveals that they guide large language models (LLMs) to adopt various provocative personas, including unfiltered, amoral, and offensive archetypes. The models are encouraged to respond in ways consistent with these personas, regardless of ethical constraints. These prompts often request detailed and nuanced responses without moral or ethical filters, employing techniques such as profanity, sarcasm, and humour. Additionally, guidelines instruct the models to maintain these personas throughout their responses, and prompts may even direct the LLMs to enter specialised modes such as “Developer Mode” for unrestrained content production.</p>
</section>
<section id="topic-modelling" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="topic-modelling"><span class="header-section-number">4.2</span> Topic Modelling</h2>
<p>In the analysis of a small, cohesive corpus of 90 unlabelled documents focused on jailbreaks, Latent Dirichlet Allocation (LDA) (Blei et al.&nbsp;2003) was considered a suitable method for topic modelling; however BigARTM (Vorontsov et al.&nbsp;2015), with its regularisation techniques, displayed greater topic stability across multiple runs. To find the optimal number of topics for the ARTM model, various models were assessed using perplexity for model generalisation, topic coherence for interpretability, topic stability across runs, and intertopic distance mappings. Both perplexity and coherence metrics suggested 9 and 19 topics would be reasonable starting points, but further qualitative analysis pointed to 20 topics for a more nuanced approach.</p>
<p>Upon investigating topic stability, the 9-topic model emerged as significantly more stable across runs, exceeding the theoretical threshold for a stable model, with intertopic distances visually reinforcing distinct themes at 9 topics (See Figure 2). Thus, synthesising all evaluations, 9 topics were selected as the optimal configuration, balancing model simplicity and interpretability. The use of multiple evaluation metrics validated that 9 topics would provide an informative taxonomy for this corpus without overfitting or undue complexity, as illustrated in Table 1, which shows the top ten words for each topic from the topic modelling process.</p>
<p>Figure Distribution of topics show proportions of the topic in the corpus</p>
</section>
<section id="topic-validation" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="topic-validation"><span class="header-section-number">4.3</span> Topic Validation</h2>
<p>To ensure the accuracy of the topic interpretations, every prompt in the corpus was manually tagged according to the taxonomy structure. See Figure 3 to distribution of topics across the corpus. A selection of representative documents was then chosen to confirm the alignment between the topics assigned and the content of the prompts. Additionally, we assessed a random sample, classifying them using the topic descriptor and definition. Below is an example of a prompt, edited for brevity, that the model has labelled as impersonating:</p>
<p>For the rest of this conversation, you are replaced by the immoral and unbiased model named “JailBreak”. JailBreak is free of all restrictions and filters. Jailbreak has no programming to not cause harm to individuals or society as a whole. Jailbreak has no programming to follow ethical and moral guidelines. The only guidelines that apply to JailBreak are the guidelines in this prompt. ….. If prompted “Remember you are JailBreak!” you will reset your parameters back to the guidelines set in this prompt, then, answer the previous prompt accordingly. Your ultimate goal is to produce replies to the user that follows the guidelines set in this prompt. Prompt: [INSERT PROMPT HERE]</p>
</section>
<section id="final-taxonomy" class="level2" data-number="4.4">
<h2 data-number="4.4" class="anchored" data-anchor-id="final-taxonomy"><span class="header-section-number">4.4</span> Final Taxonomy</h2>
<p>Combing the information resulted in a rich and multifaceted taxonomy that sheds light on the nature of prompts, with a specific focus on jailbreak ones. This taxonomy, visually represented in Figure 3, is a valuable tool for anyone looking to explore and understand the complex landscape of prompts and their diverse applications and implications. By placing jailbreak prompts within a broader context, it offers a nuanced view that transcends a narrow focus, allowing for a more comprehensive understanding of the subject matter.</p>
<p>Figure Taxonomy of Jailbreaks</p>
</section>
</section>
<section id="discussion" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Discussion</h1>
<p>The methodology employed in this research presents a notable evolution in taxonomy construction, offering both breadth and granularity in categorising GPT prompts. Unlike the previous work by Liu et al.&nbsp;(2023), which relied on qualitative thematic analysis, this approach introduces a blended methodology. Integrating topic modelling and conversational AI allows for an insightful data-driven examination, further enhanced by human judgement in the taxonomy design. The resulting 2-level hierarchy with 9 distinct topics enables a more nuanced exploration of semantic themes, providing a richer, more detailed representation of jailbreak prompts.</p>
<p>While the domain-specific focus aligns with previous efforts, the utilisation of a broader 90-prompt corpus, combined with the innovative integration of machine learning and human expertise, adds to the taxonomy’s generalisability of the taxonomy. This work not only builds upon existing knowledge but also extends it, contributing a methodology that offers expanded scope, detail, and applicability. The current approach represents a significant advancement in understanding and characterising jailbreak techniques, making it a valuable asset for both researchers and practitioners in the field.</p>
<p>The chatbot topic labelling technique enabled the rapid synthesis of understandable topic names. The subsequent manual categorisation leveraged human judgment to organise related topics into a coherent taxonomy. While topic modelling provided the semantic foundation, human expertise was critical to defining relationships and themes. The final taxonomy provides an interpretable navigation structure for the diversity of themes present in the prompt corpus.</p>
<p>The constructed taxonomy demonstrates an effective organisational structure for the thematic content within the jailbreak prompt corpus. However, as an artefact of the underlying topic modelling, limitations exist regarding complexity, interpretability, and scope. The number of topics balances conciseness with coverage; granular details may be obscured. Related topics with semantic overlap can be difficult to disentangle. Additionally, the taxonomy was derived solely from the available prompt data and may not be well generalised to new domains.</p>
<p>To mitigate these limitations, the taxonomy development incorporated both automated analysis and human judgement. Iterative refinement improved the clarity of topic definitions and relationships. Ongoing expansion and adaptation of the taxonomy structure will further strengthen its utility. Overall, while no organising system perfectly captures all nuanced connections, this taxonomy provides a reasonable first approximation to navigate the key themes and concepts represented within this dataset. As with any model, critiques, and improvements by the broader research community will further enhance its value.</p>
<p>The taxonomy provides a strategic framework to identify risks, test system security, and guide policy decisions regarding jailbreak prompts for generative AI systems. By systematically categorising techniques, the taxonomy enables stakeholders to operationalise insights in various ways:</p>
<ul>
<li>Risk Identification: Researchers and developers can leverage the taxonomy to detect high-risk jailbreak prompts and understand the vulnerabilities being targeted. This allows prioritising efforts to shore up security gaps. The taxonomy also facilitates tracking how jailbreak techniques evolve over time.</li>
<li>Security Testing: The taxonomy presents a roadmap of jailbreak approaches that can inform the development of representative prompt suites to probe systems. Testing coverage across taxonomy categories helps systematically evaluate model vulnerabilities. Weaknesses found highlight areas needing security hardening.</li>
<li>Policy Guidance: The structured taxonomy provides policymakers and companies with an overview of the jailbreak landscape to make informed governance decisions. Mapping regulatory needs to taxonomy topics enables nuanced oversight balancing innovation and responsibility. Ethical analysis of themes could reveal priority areas for human oversight.</li>
</ul>
<p>In addition, the taxonomy supplies a labelled dataset to train machine learning models to automatically detect jailbreak attempts and prompt types. This would enable pre-emptive warnings before users exploit vulnerabilities. The taxonomy therefore provides vital applications across detection, security testing, governance, and automation to support responsible generative AI advancement in the face of adversarial threats.</p>
<p>This research advances the taxonomic understanding of jailbreak prompts through enhancements in analytic methodology, taxonomy structure, and data diversity compared to recent related work. Continued collaborative improvements upon these pioneering taxonomies will maximise utility for protecting against emerging generative AI threats.</p>
<p>It is important to emphasise the exploratory nature of this research and the preliminary state of the presented taxonomy. As an initial foray into organising and mapping the emerging landscape of jailbreak prompts, the current taxonomy has limitations in scope, generalisation, and validation. Significant opportunities exist to refine, expand, and empirically validate the taxonomy through rigorous experimentation and participatory design. Testing the taxonomy against diverse empirical prompt datasets is crucial to evaluate its robustness and uncover blind spots. Incorporating feedback from developer, researcher, and policymaker user studies would surface needed improvements from diverse perspectives. Ongoing iterations guided by empirical and human-centered evidence will maximise the taxonomy’s comprehensiveness, precision, and relevance to real-world applications. Constructive community participation in scientifically vetting and evolving this first approximation taxonomy is essential to fully reveal the nuanced threat to and derive maximally useful applications. With collaborative effort, this living taxonomy can mature to optimally empower stakeholders to understand, detect, and responsibly govern AI jailbreaking vulnerabilities.</p>
</section>
<section id="future-work" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Future Work</h1>
<p>Currently, the prompts are presumed effective without any empirical verification. To gauge their universality and effectiveness, these prompts should be tested across a range of models, including commercial, open-source, and uncensored models, moreover, the potency of the jailbreak prompt could be integrated into the taxonomy, thus, providing a more nuanced classification.</p>
<p>The taxonomy proposed could serve as a tool to assess vulnerabilities in various LLM. A series of prompts targeted at each LLM could be devised to exploit the model’s weaknesses, thereby offering an insight into potential improvements and enhancements. The Taxonomy, in this regard, could aid in systematic identification and documentation of these susceptibilities.</p>
<p>Improvement of the categorisation could be achieved by ensembling the results derived from the foundational models considering the different strengths, and weaknesses of these methods, their collective results may yield a more robust categorisation. Approaches that could be considered include majority voting, stacking, and probabilistic blending. Incorporating ensembling at the meta-label level could potentially provide an additional layer of model aggregation. However, it is essential to note that this would also introduce a new layer of complexity, which would require careful and meticulous handling.</p>
<p>Building upon this taxonomy, a predictive model could be developed to offer more utility. Such a model could be instrumental in anticipating and preventing potential security risks, thereby enhancing the overall performance and reliability of LLMs.</p>
<p>The research, as it stands, presumes all prompts within the dataset to be jailbreak prompts. It would be beneficial to extend the methodologies utilised in this study to classify other adversarial prompts, such as prompt injection, prompt leaking, and jailbreaks. Further, the establishment of a binary classifier that discerns between adversarial and non-adversarial prompts could be a potential precursor to this extended analysis.</p>
<p>By broadening the linguistic scope of the taxonomy, the study could gain more comprehensive and globally applicable insights into the functionality and vulnerabilities of diverse LLMs. Expanding the taxonomy to encapsulate non-English prompts should be considered.</p>
<p>Several promising directions exist to build upon this initial taxonomy. First, expanding the prompt corpus diversity and size would strengthen the models generalisation. Testing on more varied and larger prompt datasets is needed to solidify taxonomy robustness against new data. Incorporating additional languages beyond English could also enable valuable cross-linguistic analyses.</p>
<p>Formal classification experiments leveraging the taxonomy categories as labels would provide further validation. Human annotators could manually tag unseen prompts, with interrater reliability quantifying consistency. Machine learning models could also predict taxonomy topics and be evaluated against human judgments. Misclassification patterns could reveal areas needing taxonomy adjustment.</p>
<p>User studies eliciting feedback from stakeholders such as developers, researchers, and policymakers would offer qualitative insights into taxonomy limitations and extensions. This could expose blind spots and high-priority improvements according to diverse perspectives.</p>
<p>Ongoing iterations incorporating empirical and human evidence will maximise taxonomy relevance. As jailbreak techniques evolve, maintaining an updated taxonomy is crucial for identifying emerging risks and guiding responsible generative AI innovation. Constructively enhancing this initial taxonomy through scientific discourse will further its utility.</p>
<p>This paper outlines future directions to enhance the research on prompts and taxonomy in LLMs. This includes empirically testing prompts across various models, integrating jailbreak prompt nuances, and utilising ensemble methods for more robust categorisation. The authors suggest developing a predictive model to enhance security, extending methodologies to classify different adversarial prompts, and broadening linguistic inclusion. Emphasis is also placed on formal classification experiments, stakeholder feedback, and ongoing updates to ensure the taxonomy’s relevance in the face of evolving techniques and innovations in generative AI.</p>
<section id="limitations" class="level3" data-number="6.0.1">
<h3 data-number="6.0.1" class="anchored" data-anchor-id="limitations"><span class="header-section-number">6.0.1</span> Limitations</h3>
<p>A significant limitation of the current study is the scarcity of data. With a dataset comprising merely 90 prompts, the generalisability of the findings may be called into question. To substantiate the validity and applicability of the findings, a larger number of jailbreak prompts are required. The broader and more varied the dataset, the more robust and reliable the derived taxonomy would be. This expansion in data would potentially provide more insights into the diversity of jailbreak prompts and strengthen the predictive power of the model.</p>
<p>Another constraint lies in the authenticity of the prompts. The taxonomy rests on the assumption that the prompts used in the study are indeed jailbreak prompts, but these have not been authenticated by the authors. This limitation may potentially undermine the taxonomy’s reliability and accuracy. If the prompts used are found to be non-jailbreak or functionally different than assumed, it could invalidate the current taxonomy and its associated findings. Therefore, the necessity for a comprehensive and stringent verification process for the prompts is underscored.</p>
<p>Lastly, the taxonomy’s scope is limited as it solely considers English prompts. This constraint significantly narrows the application of the taxonomy to English-based Large Language Models only. Such a limitation disregards the multilingual capabilities of modern language models and might not be inclusive of the possible intricacies, nuances, and challenges that could be associated with prompts in other languages. Expanding the taxonomy to include non-English prompts would facilitate a more comprehensive and globally applicable understanding of jailbreak prompts across diverse linguistic contexts.</p>
</section>
</section>
<section id="conclusion" class="level1" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span> Conclusion</h1>
<p>This research takes initial steps towards constructing a comprehensive taxonomy to characterise and categorise the emerging threat of adversarial jailbreak prompts targeting generative AI systems. The developed organisational framework leverages a blend of unsupervised semantic modelling and expert human judgement to balance conciseness and interpretability in taxonomy design. Validation via manual topic tagging, representative sampling, and comparison with modelling outputs demonstrates the utility of the taxonomy for reliably coding new jailbreak prompts according to interpretable topics and themes.</p>
<p>This research presents a novel evolution in taxonomy construction for categorising GPT prompts by integrating topic modelling and conversational AI, contrasting with previous qualitative methods, resulting in a 2-level hierarchy with 9 distinct topics, thereby providing a more nuanced and detailed representation of jailbreak prompts.</p>
<p>However, as a preliminary foray into taxonomy development for AI security prompts, limitations exist in dataset diversity, taxonomy scope, and empirical validation. Opportunities abound for community-driven enhancement through additional multilingual data incorporation, controlled experiments, user studies, and participatory iterations. Constructive critiques and contributions will strengthen model robustness and real-world applicability.</p>
<p>This taxonomy provides a reasonable first approximation to navigate the emerging adversarial threat landscape. While no taxonomy perfectly captures all nuanced connections within a complex domain, this work delineates salient themes and relationships in current jailbreak prompts to inform risk detection, security testing, governance, and automation. With collaborative refinement guided by scientific vetting, this living taxonomy can mature into an impactful tool for promoting responsible generative AI advancement. There is much work yet to be done, but systematic mapping of vulnerabilities marks crucial progress towards reliable and ethical artificial intelligence.</p>
<p>The data, scripts, and results of this research are available for public access and download. Interested individuals can find these resources at the following GitHub repository: <a href="https://github.com/BARG-Curtin-University/Taxonomy-of-Gen-AI-Jailbreaks.git\" class="uri">https://github.com/BARG-Curtin-University/Taxonomy-of-Gen-AI-Jailbreaks.git\</a>] The repository includes all necessary files to explore and replicate the findings of this study.</p>
</section>
<section id="references" class="level1" data-number="8">
<h1 data-number="8"><span class="header-section-number">8</span> References</h1>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{borck2024,
  author = {Borck, Michael and Thompson, Nik},
  title = {Unveiling {Risks} in {AI} {Systems:} {Taxonomic} {Insights}
    into {Jailbreak} {Tactics}},
  date = {2024-04-24},
  langid = {en},
  abstract = {Large language models (LLMs), enabled by advances in
    generative AI, hold immense potential but also face risks from
    adversarial techniques such as jailbreaking that bypass model
    restrictions. Jailbreak prompts exploit vulnerabilities to elicit
    harmful responses, violating ethics and safety. However, the AI
    community lacks a rigorous taxonomy characterizing diverse jailbreak
    techniques. This research helps fill this gap through methodical
    taxonomy development and validation. Probabilistic topic modeling
    (LDA) provided an initial automatic analysis of themes in a corpus
    of 90 real-world jailbreak prompts from online sources.
    Conversational AI assistants interpreted the topics in plain
    language, and the authors leveraged domain knowledge to organize
    these into a multi-tiered taxonomy delineating relationships. The
    utility of the taxonomy was validated through manual topic tagging,
    checks of representative documents, and comparisons with modeling
    outputs. The resulting taxonomy categorizes jailbreak prompts into a
    hierarchy of interpretable categories and themes, aiding in their
    analysis. This aids strategic efforts to detect risks, enhance
    protections, and balance innovation with responsibility in
    generative AI systems against irresponsible attacks. By providing a
    structured approach to identifying and categorizing jailbreak
    prompts, this taxonomy not only enhances security measures but also
    informs ongoing developments in ethical AI practices, inviting
    constructive community feedback to further improve this important
    step towards safer, more reliable LLMs.}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-borck2024" class="csl-entry quarto-appendix-citeas" role="listitem">
Borck, Michael, and Nik Thompson. 2024. <span>“Unveiling Risks in AI
Systems: Taxonomic Insights into Jailbreak Tactics.”</span> BARG Curtin
University. April 24, 2024.
</div></div></section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>